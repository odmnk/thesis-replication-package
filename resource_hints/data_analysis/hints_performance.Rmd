---
title: "R Notebook"
output: html_notebook
---
# Resource Hints Performance
# Housekeeping

First we set the working directory
```{r}
dir = "/home/omar/thesis-replication-package/resource_hints/data_analysis"
```

```{r setup, include=FALSE, cache = FALSE}
require("knitr")
## setting working directory
opts_knit$set(root.dir = dir)
```

```{r}
setwd(dir)
```

Next we import the libraries we need.
```{r}
library(ggplot2)
library(e1071)
library(dplyr)
library(effsize)
# Import functins we created outselves.
source("util.r")
```

# Getting the data
We load the data and then reshape it a little bit to make it easier to work with.
```{r}
monsoon_raw_data <- read.csv("../raw_data/Aggregated_Results_Monsoon.csv")
plt_raw_data <- read.csv("../raw_data/http_post_request_payloads.txt")
combined_data = combine_and_reshape_dataframes(monsoon_raw_data, plt_raw_data, "noremovehints")
intact = combined_data[1:200,]
removed = combined_data[201:400,]
print(combined_data)
print(intact)
print(removed)
```

# Data Exploration

First lets get some summaries of the data..
```{r}
print("All")
summary(combined_data$plt)
sd(combined_data$plt)
print("Intact")
summary(intact$plt)
sd(intact$plt)
print("Removed")
summary(removed$plt)
sd(removed$plt)
```
Then we make a boxplot of the data.
```{r}
ggplot(combined_data, aes(x=treatment, y=plt)) + 
  geom_boxplot()
```
We can see that xx are very close.


Lets inspect the  outliers..
```{r}
outliers <- boxplot.stats(combined_data$plt)$out
out_ind <- which(combined_data$plt %in% c(outliers))
outliers <- combined_data[out_ind,]
print(outliers)
outliers %>% group_by(subject, treatment) %>% summarise(Count=n())
```
We see that most of the outliers are all for the same mobile web apps (30/35 outliers) so we decide to not remove them. This is also to be expected as each mobile web app differs from the others in terms of size, logic etc.

# Testing for normality
The paired t-test assumes that the differences between pairs are normally distributed. Lets investigate..
```{r}
differences <- NULL
differences$subject <- intact$subject
differences$diff <-  removed$plt - intact$plt
differences <- data.frame(differences$subject, differences$diff)
differences <- differences %>%
  rename(
   subject = "differences.subject",
    diff = differences.diff
    )
print(differences)
summary(differences$diff)
stripchart(differences$diff)
boxplot(differences$diff)
```
We see quite a big difference in the PLT.
Might be interesting to look at the outliers. As we can see the majority of these outliers are all from the same subjects.

```{r}
outliers <- boxplot.stats(differences$diff)$out
out_ind <- which(differences$diff %in% c(outliers))
differences[out_ind,]
```


Lets check whether the data is normally distributed. The QQ-plot indicates that the data is not normally distributed. To confirm we apply he Shapiro-Wilk test which returns a p-value of  2.2e-16 meaning that the hypothesis that the data comes from a normal distribution is rejected. 

```{r}
transform_and_test(removed$plt, intact$plt)
```

Since the differences are not normally distributed we apply several transformations to see if we can approach a normal distribution.

```{r}
transform_and_test(removed$plt, intact$plt, "log", reverse=TRUE)
transform_and_test(removed$plt, intact$plt, "sqrt", reverse=TRUE)
transform_and_test(removed$plt, intact$plt, "recip", reverse=TRUE)
```

We can see that for all three transformations the data is not normally distributed.

# Hypothesis Testing
Since the data is not normally distributed we  will use the Wilcoxon signed rank test:
```{r}
wilcox.test(removed$plt, intact$plt, paired=TRUE)
```
As we can see this tests gives a p-value of 1.235-05 which is below our 0.05 treshold and thus we  reject the null hypothesis. So we are  able to claim that there is a significant difference in PLT between MWP that use resource preloading or not.

# Effect Size Estimation
```{r}
cliffs_delta_plt <- cliff.delta(removed$plt, intact$plt)
print(cliffs_delta_plt)
```
As expected the value of the Cliff's delta measure is negligible.





